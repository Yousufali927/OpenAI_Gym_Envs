{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca26768",
   "metadata": {},
   "source": [
    "simple 3x3 grid env, the agent starts a (1,1) and \n",
    "goal is to reach (3,3,) set of actions could be left, right, up, down \n",
    "depending on which state agent is in. rewards = -1 for all states \n",
    "except terminal, for which it is 103 optimal policy reward = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcc7bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /var/folders/4s/rj_sy56d06508gj9z8g3lwhw0000gn/T/matplotlib-j0v0bx7w because the default path (/Users/yousuf/.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym \n",
    "from gym import Env\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "action space\n",
    "0 - left\n",
    "1 - up\n",
    "2 - right\n",
    "3 - down\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c071de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze_3x3_Grid(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(9)\n",
    "        self.state = (0, 0)\n",
    "        self.episode_length = 10\n",
    "\n",
    "        \n",
    "    def step(self,action):\n",
    "        \n",
    "        if self.state==8:\n",
    "            return self.state, 0, True, {}\n",
    "        \n",
    "        self.episode_length -= 1\n",
    "        \n",
    "        # Converting the number to (row, col) to check the logic\n",
    "        row, col = divmod(self.state, 3)\n",
    "        \n",
    "        if action == 0 and col > 0:  # Left\n",
    "            col -= 1\n",
    "        elif action == 1 and col < 2:  # Right\n",
    "            col += 1\n",
    "        elif action == 2 and row > 0:  # Up\n",
    "            row -= 1\n",
    "        elif action == 3 and row < 2:  # Down\n",
    "            row += 1\n",
    "        \n",
    "        # Updating state by converting (row, col) back to number state\n",
    "        self.state = row * 3 + col\n",
    "        \n",
    "        if self.state == 8:\n",
    "            reward = 103\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "            \n",
    "        if self.episode_length<=0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "            \n",
    "\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "        \n",
    "        \n",
    "    def render(self):\n",
    "        '''\n",
    "        grid = np.zeros((3, 3), dtype=str)\n",
    "        grid.fill('-')\n",
    "        row, col = divmod(self.state, 3)\n",
    "        grid[row, col] = 'A'  # This will mark the agents posiition in the grid\n",
    "        print(grid)\n",
    "        print(\"\\n\")\n",
    "        '''\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = 0  # resetting to start position\n",
    "        self.episode_length = 10\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5069fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze_3x3_Grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa0cd1",
   "metadata": {},
   "source": [
    "### Testing Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69af13fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-10\n",
      "Episode:2 Score:94\n",
      "Episode:3 Score:-10\n",
      "Episode:4 Score:96\n",
      "Episode:5 Score:95\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    # step = 1\n",
    "    while not done:\n",
    "        \n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        ## print(step)\n",
    "        ## step+=1\n",
    "        \n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fc6fc",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639e8050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousuf/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3197d96b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.64     |\n",
      "|    ep_rew_mean     | 12.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 6565     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.07        |\n",
      "|    ep_rew_mean          | 36          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4468        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020304106 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.000976   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 251         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    value_loss           | 1.05e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.78        |\n",
      "|    ep_rew_mean          | 50          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4162        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018088423 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.0171      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 918         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 1.83e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.16         |\n",
      "|    ep_rew_mean          | 65.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4046         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0141449645 |\n",
      "|    clip_fraction        | 0.177        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0.0321       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 969          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0335      |\n",
      "|    value_loss           | 2.08e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.5         |\n",
      "|    ep_rew_mean          | 80.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3979        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012705723 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.0819      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.18e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0331     |\n",
      "|    value_loss           | 2.3e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.67        |\n",
      "|    ep_rew_mean          | 89.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3938        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011060292 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 952         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 2.31e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.2         |\n",
      "|    ep_rew_mean          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3901        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016671227 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 915         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    value_loss           | 2.02e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.75        |\n",
      "|    ep_rew_mean          | 98.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3871        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014481414 |\n",
      "|    clip_fraction        | 0.0856      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.939      |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 770         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 1.68e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.29        |\n",
      "|    ep_rew_mean          | 99.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3840        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017633434 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.849      |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 604         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 1.32e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.3         |\n",
      "|    ep_rew_mean          | 99.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3816        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014357144 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.777      |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 412         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00882    |\n",
      "|    value_loss           | 984         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.26        |\n",
      "|    ep_rew_mean          | 99.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3765        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016240537 |\n",
      "|    clip_fraction        | 0.0672      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.748      |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 314         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00887    |\n",
      "|    value_loss           | 687         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.18         |\n",
      "|    ep_rew_mean          | 99.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3751         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050467644 |\n",
      "|    clip_fraction        | 0.0253       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.714       |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 164          |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00301     |\n",
      "|    value_loss           | 447          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.06        |\n",
      "|    ep_rew_mean          | 99.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3754        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014291562 |\n",
      "|    clip_fraction        | 0.0519      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 95.6        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00385    |\n",
      "|    value_loss           | 259         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.07        |\n",
      "|    ep_rew_mean          | 99.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3750        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011690918 |\n",
      "|    clip_fraction        | 0.0877      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.655      |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.7        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00561    |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.04       |\n",
      "|    ep_rew_mean          | 100        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3744       |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01879511 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.631     |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.57       |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    value_loss           | 38.4       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.01         |\n",
      "|    ep_rew_mean          | 100          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3738         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063741095 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.617       |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.8          |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00548     |\n",
      "|    value_loss           | 7.03         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.01        |\n",
      "|    ep_rew_mean          | 100         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3737        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008929152 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.606      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.000859   |\n",
      "|    value_loss           | 4.39        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.01         |\n",
      "|    ep_rew_mean          | 100          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3734         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047823703 |\n",
      "|    clip_fraction        | 0.0586       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.594       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00432      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | 0.000106     |\n",
      "|    value_loss           | 1.47         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.02        |\n",
      "|    ep_rew_mean          | 100         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3725        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005981164 |\n",
      "|    clip_fraction        | 0.083       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00488    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00683    |\n",
      "|    value_loss           | 0.0429      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.04        |\n",
      "|    ep_rew_mean          | 100         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3725        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016760906 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.533      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 0.0532      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 100         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3718        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009650457 |\n",
      "|    clip_fraction        | 0.0525      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.532      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00404    |\n",
      "|    value_loss           | 0.0747      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 100         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3714        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016988406 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0196     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    value_loss           | 0.0134      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 100         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3712        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008362016 |\n",
      "|    clip_fraction        | 0.0687      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.511      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00604    |\n",
      "|    value_loss           | 0.0128      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5            |\n",
      "|    ep_rew_mean          | 100          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3709         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067857183 |\n",
      "|    clip_fraction        | 0.0384       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.496       |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00951     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    value_loss           | 0.00369      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 100         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3706        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012627857 |\n",
      "|    clip_fraction        | 0.0674      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.511      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0274     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.000669   |\n",
      "|    value_loss           | 0.0149      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x292ec78d0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375eccb",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e573049",
   "metadata": {},
   "outputs": [],
   "source": [
    "Maze_Path = os.path.join('Training', 'Saved Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(Maze_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdfe716",
   "metadata": {},
   "source": [
    "### Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3834184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 100.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousuf/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "/Users/yousuf/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, render=True)\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065cac04",
   "metadata": {},
   "source": [
    "### Let's see the agent in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "741cff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the class Again cuz I didnt want to make changed to the original class \n",
    "## i.e uncommenting the render function cuz If I did that when U re-train the model the whole rendering would just \n",
    "## slow down the training\n",
    "\n",
    "\n",
    "class Maze_3x3_Grid(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(9)\n",
    "        self.state = (0, 0)\n",
    "        self.episode_length = 10\n",
    "\n",
    "        \n",
    "    def step(self,action):\n",
    "        \n",
    "        if self.state==8:\n",
    "            return self.state, 0, True, {}\n",
    "        \n",
    "        self.episode_length -= 1\n",
    "        \n",
    "        # Converting the number to (row, col) to check the logic\n",
    "        row, col = divmod(self.state, 3)\n",
    "        \n",
    "        if action == 0 and col > 0:  # Left\n",
    "            col -= 1\n",
    "        elif action == 1 and col < 2:  # Right\n",
    "            col += 1\n",
    "        elif action == 2 and row > 0:  # Up\n",
    "            row -= 1\n",
    "        elif action == 3 and row < 2:  # Down\n",
    "            row += 1\n",
    "        \n",
    "        # Updating state by converting (row, col) back to number state\n",
    "        self.state = row * 3 + col\n",
    "        \n",
    "        if self.state == 8:\n",
    "            reward = 103\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "            \n",
    "        if self.episode_length<=0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "            \n",
    "\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "        \n",
    "        \n",
    "    def render(self):\n",
    "        \n",
    "        grid = np.zeros((3, 3), dtype=str)\n",
    "        grid.fill('-')\n",
    "        row, col = divmod(self.state, 3)\n",
    "        grid[row, col] = 'A'  # This will mark the agents posiition in the grid\n",
    "        print(grid)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = 0  # resetting to start position\n",
    "        self.episode_length = 10\n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c146afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze_3x3_Grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1e8d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(Maze_Path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c91b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 5 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' 'A' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 10 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' 'A' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' 'A' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 15 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 20 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' 'A' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 25 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' 'A' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 30 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' 'A' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 35 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 40 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 45 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 50 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 55 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 60 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 65 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['A' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' 'A' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 70 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 75 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' 'A' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 80 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 85 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 90 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['A' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 95 steps with 100 Reward\n",
      "[['A' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' 'A' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' 'A' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' 'A']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "\n",
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'A']]\n",
      "\n",
      "\n",
      "Episode finished after 100 steps with 100 Reward\n"
     ]
    }
   ],
   "source": [
    "env = Maze_3x3_Grid()  \n",
    "obs = env.reset()\n",
    "\n",
    "for step in range(100):  \n",
    "    \n",
    "    env.render()\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    score+=reward\n",
    "    if done:\n",
    "        print(f\"Episode finished after {step+1} steps with {score} Reward\")\n",
    "        obs = env.reset()\n",
    "        score = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf29e2a",
   "metadata": {},
   "source": [
    "## We can see that the agent has learned the optimal Policy and is executing the same everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0aade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b515c0a",
   "metadata": {},
   "source": [
    "## Let's Check the Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93a4f5c9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.66.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (69.5.1)\n",
      "Requirement already satisfied: six>1.9 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading grpcio-1.66.0-cp312-cp312-macosx_10_9_universal2.whl (10.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: tensorboard-data-server, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 grpcio-1.66.0 tensorboard-2.17.1 tensorboard-data-server-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e4a8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c8b0b6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3d4169700ace4e9a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3d4169700ace4e9a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./Training/Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51e802",
   "metadata": {},
   "source": [
    "WE can see that at around 40,000 Steps we achieved the optimal policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
